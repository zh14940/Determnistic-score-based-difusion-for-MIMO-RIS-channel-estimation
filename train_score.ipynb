{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8914a8b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T11:36:15.310843Z",
     "start_time": "2023-11-13T11:36:14.814201100Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE =16\n",
    "torch.no_grad()\n",
    "\n",
    "# with h5py.File('HdLink.mat', 'r') as file:\n",
    "#     hdlink_data = file['HdLink']\n",
    "#     hdlink_real = np.array(hdlink_data['real'])  \n",
    "#     hdlink_imag = np.array(hdlink_data['imag']) \n",
    "#     Hdlink = torch.complex(torch.FloatTensor(hdlink_real), torch.FloatTensor(hdlink_imag))\n",
    "                           \n",
    "# with h5py.File('HrLink.mat', 'r') as file:\n",
    "#     hrlink_data = file['HrLink']\n",
    "#     hrlink_real = np.array(hrlink_data['real'])  \n",
    "#     hrlink_imag = np.array(hrlink_data['imag']) \n",
    "#     Hrlink = torch.complex(torch.FloatTensor(hrlink_real), torch.FloatTensor(hrlink_imag))\n",
    "    \n",
    "Htlink_data = loadmat('HtLink.mat')\n",
    "Htlink = torch.from_numpy(Htlink_data['HtLink'])\n",
    "Hrlink_data = loadmat('HrLink.mat')\n",
    "Hrlink = torch.from_numpy(Hrlink_data['HrLink'])\n",
    "Hdlink_data = loadmat('HdLink.mat')\n",
    "Hdlink = torch.from_numpy(Hdlink_data['HdLink'])\n",
    "\n",
    "#del hdlink_data,hdlink_real,hdlink_imag,hrlink_data,hrlink_real,hrlink_imag,Htlink_data\n",
    "\n",
    "Gchannel = torch.zeros(Hdlink.shape[0],Hdlink.shape[1],2,Hrlink.shape[2])\n",
    "phi = 2*(torch.rand(Hrlink.shape[1])+1j*torch.rand(Hrlink.shape[1]))-1\n",
    "diag_phi = torch.diag(phi).to(torch.complex128)\n",
    "for i in range(Hrlink.shape[2]):\n",
    "    nchannel = Hdlink[:,:,i] + Hrlink[:,:,i] @ diag_phi @ Htlink\n",
    "    Gchannel[:,:,0,i] = torch.real(nchannel/torch.abs(nchannel))\n",
    "    Gchannel[:,:,1,i] = torch.imag(nchannel/torch.abs(nchannel))\n",
    "#Gchannel = Gchannel/torch.abs(Gchannel)\n",
    "torch.save(Gchannel, 'Gchannel.pt')\n",
    "dataloader = DataLoader(Gchannel.view(-1,2,IMG_SIZE,IMG_SIZE), batch_size=BATCH_SIZE, shuffle=True, drop_last=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9a57c18f95ae30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T11:36:22.436783500Z",
     "start_time": "2023-11-13T11:36:21.971029500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  62438242\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 2\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 2\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "model = SimpleUnet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad203113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.18299102783203\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'timestep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      8\u001b[0m flops \u001b[38;5;241m=\u001b[39m FlopCountAnalysis(model, input_tensor)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mflops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\fvcore\\nn\\jit_analysis.py:248\u001b[0m, in \u001b[0;36mJitModelAnalysis.total\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    Returns the total aggregated statistic across all operators\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    for the requested module.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m        int : The aggregated statistic.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_module_name(module_name)\n\u001b[0;32m    250\u001b[0m     total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(stats\u001b[38;5;241m.\u001b[39mcounts[module_name]\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\fvcore\\nn\\jit_analysis.py:551\u001b[0m, in \u001b[0;36mJitModelAnalysis._analyze\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_trace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_tracer_warning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    550\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mTracerWarning)\n\u001b[1;32m--> 551\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_get_scoped_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aliases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Assures even modules not in the trace graph are initialized to zero count\u001b[39;00m\n\u001b[0;32m    554\u001b[0m counts \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\fvcore\\nn\\jit_analysis.py:176\u001b[0m, in \u001b[0;36m_get_scoped_trace_graph\u001b[1;34m(module, inputs, aliases)\u001b[0m\n\u001b[0;32m    173\u001b[0m     name \u001b[38;5;241m=\u001b[39m aliases[mod]\n\u001b[0;32m    174\u001b[0m     register_hooks(mod, name)\n\u001b[1;32m--> 176\u001b[0m graph, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m hook_handles:\n\u001b[0;32m    179\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:1175\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   1174\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m-> 1175\u001b[0m outs \u001b[38;5;241m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 127\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\jit\\_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    117\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 118\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    120\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1116\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1118\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'timestep'"
     ]
    }
   ],
   "source": [
    "model_size_bytes = sum(p.numel() for p in model.parameters()) * 4  # numel()返回参数的总元素数，乘以4字节得到总字节数\n",
    "model_size_mb = model_size_bytes / (1024 ** 2)  # 将字节转换为兆字节\n",
    "print(model_size_mb)\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# model是你的网络模型，input是一个典型的输入张量\n",
    "input_tensor = torch.randn(32, 2, 64, 64)\n",
    "flops = FlopCountAnalysis(model, input_tensor)\n",
    "print(flops.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd183a3b9b836d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T11:36:29.564718900Z",
     "start_time": "2023-11-13T11:36:29.544774800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "############   Loss function & Sampling\n",
    "\n",
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.l1_loss(noise, noise_pred)\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    Calls the model to predict the noise in the image and returns \n",
    "    the denoised image. \n",
    "    Applies noise to this image, if we are not in the last step yet.\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE ###############\n",
    "    img = torch.randn((1, 2, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]: ####################\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        # Edit: This is to maintain the natural range of the distribution\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize)+1)\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dcabd54660f7f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T11:36:33.060369100Z",
     "start_time": "2023-11-13T11:36:31.312045800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhizhou He\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Zhizhou He\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#### Forward process\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\" \n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\" \n",
    "    Takes an image and a timestep as input and \n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce7a7ba9f811d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T11:36:38.673357300Z",
     "start_time": "2023-11-13T11:36:38.651415800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | step 000 Loss: 0.8127553462982178 \n",
      "Epoch 5 | step 000 Loss: 0.3347211480140686 \n",
      "Epoch 10 | step 000 Loss: 0.37774068117141724 \n",
      "Epoch 15 | step 000 Loss: 0.2364252805709839 \n",
      "Epoch 20 | step 000 Loss: 0.2443949431180954 \n",
      "Epoch 25 | step 000 Loss: 0.16808025538921356 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, T, (BATCH_SIZE,), device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;66;03m# sample random time slot index from T\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_loss(model, batch, t)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100 # Try more!\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "      t = torch.randint(0, T, (BATCH_SIZE,), device=device).long() # sample random time slot index from T\n",
    "      loss = get_loss(model, batch, t)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if epoch % 5 == 0 and step == 0:\n",
    "        print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "        #sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a71bcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T11:57:45.968117400Z",
     "start_time": "2023-11-13T11:57:45.961135700Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f77cb8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 2, 96])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "Gchannel = torch.load('Gchannel.pt')\n",
    "print(Gchannel.shape)\n",
    "########### pilot signals ############\n",
    "num_pilots = 100\n",
    "Nt = 64\n",
    "\n",
    "# 创建包含给定复数的实部和虚部的张量\n",
    "constellation_real = torch.tensor([1, 1, -1, -1], dtype=torch.float)\n",
    "constellation_imag = torch.tensor([1, -1, 1, -1], dtype=torch.float)\n",
    "\n",
    "# 生成随机索引\n",
    "p_indices = torch.randint(0, len(constellation_real), (Nt,))\n",
    "# 使用索引分别从实部和虚部张量中选择元素\n",
    "random_real_parts = constellation_real[p_indices]\n",
    "random_imag_parts = constellation_imag[p_indices]\n",
    "\n",
    "# 将实部和虚部合并成一个复数张量\n",
    "pilot = torch.complex(random_real_parts, random_imag_parts)\n",
    "print(pilot.shape)\n",
    "\n",
    "########### Pick Validation index ##############\n",
    "val_number = 16\n",
    "val_index = torch.randint(0, len(Gchannel), (val_number,))\n",
    "val_channel = torch.zeros([Nt,Nt,2,val_number])\n",
    "for i in range(len(val_index)):\n",
    "    val_channel[:,:,:,i] = Gchannel[:,:,:,val_index[i]]\n",
    "val_channel = torch.complex(torch.FloatTensor(val_channel[:,:,0,:]), torch.FloatTensor(val_channel[:,:,1,:]))\n",
    "print(val_channel.shape)\n",
    "HP = torch.zeros_like(val_channel)\n",
    "for i in range(val_channel.shape[2]):\n",
    "    HP[:,:,i] = torch.matmul(val_channel[:,:,i], pilot)\n",
    "\n",
    "########## AWGN noise ##############\n",
    "noise = torch.rand_like(HP)+1j*torch.rand_like(HP)\n",
    "snr_range = torch.arange(-10, 30, 5)\n",
    "noise_power = 10 ** (-snr_range / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1bd68b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]\u001b[A\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reshape() missing 1 required positional arguments: \"shape\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compute gradient for measurements in un-normalized space\u001b[39;00m\n\u001b[0;32m     21\u001b[0m HP_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(channel_t\u001b[38;5;241m.\u001b[39mreshape(val_number,Nt,\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mNt),pilot_real\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mNt))\n\u001b[1;32m---> 22\u001b[0m meas_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(HP_t \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview_as_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_received\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Sample noise\u001b[39;00m\n\u001b[0;32m     24\u001b[0m grad_noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m*\u001b[39m beta_noise) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(channel_t)\n",
      "\u001b[1;31mTypeError\u001b[0m: reshape() missing 1 required positional arguments: \"shape\""
     ]
    }
   ],
   "source": [
    "T = 300 ### total time slots in forward or backward process\n",
    "steps = 3 ########## iteration steps at each noise level\n",
    "random_channel = torch.randn_like(val_channel)\n",
    "random_channel = torch.view_as_real(random_channel).permute(2, 3, 0, 1)\n",
    "pilot_real = torch.view_as_real(pilot)\n",
    "################## For each SNR ###################\n",
    "for snr_idx, local_noise in tqdm(enumerate(noise_power)):\n",
    "    val_received = HP + torch.sqrt(noise_power[snr_idx])*noise # received signal through real channel\n",
    "    channel_t = random_channel.clone()\n",
    "    testing_index = 0\n",
    "    val_P_hermitian = torch.conj(torch.transpose(pilot, -1, 0))\n",
    "    P_real_hermitian = torch.conj(torch.transpose(pilot_real, -1, 0))\n",
    "    ############### For each time slot in backward process #############\n",
    "    for time_idx in tqdm(range(T)):\n",
    "        labels = torch.ones(random_channel.shape[0]) * time_idx\n",
    "        ######### For each step at that noise level ########\n",
    "        for step_idx in range(steps):\n",
    "            with torch.no_grad():\n",
    "                score = model(channel_t, labels)\n",
    "                # Compute gradient for measurements in un-normalized space\n",
    "                HP_t = torch.matmul(channel_t.reshape(val_number,Nt,2*Nt),pilot_real.reshape(2*Nt))\n",
    "                meas_grad = torch.matmul(HP_t - torch.view_as_real(val_received).reshape(),)\n",
    "                # Sample noise\n",
    "                grad_noise = np.sqrt(2 * alpha * beta_noise) * torch.randn_like(channel_t)\n",
    "                # Apply update\n",
    "                channel_t = channel_t + alpha * (score - meas_grad /(local_noise/2. + current_sigma ** 2)) + grad_noise\n",
    "                \n",
    "                # Store loss\n",
    "                nmse_log[snr_idx, testing_idx] = \\\n",
    "                (torch.sum(torch.square(torch.abs(current - oracle)), dim=(-1, -2))/\\\n",
    "                torch.sum(torch.square(torch.abs(oracle)), dim=(-1, -2))).cpu().numpy()\n",
    "                testing_idx = testing_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5983ead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64])\n",
      "torch.Size([16, 2, 64, 64])\n",
      "torch.Size([64, 64, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "score = model(batch,t)\n",
    "#print(torch.view_as_real(val_received).shape)\n",
    "print(HP_t.shape)\n",
    "print(channel_t.shape)\n",
    "print(torch.view_as_real(val_received).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
